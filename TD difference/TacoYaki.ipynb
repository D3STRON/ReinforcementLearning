{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ad6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c288bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c7ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacoYaki():\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.board = np.ones(self.size*self.size)\n",
    "        self.actions = {}\n",
    "        self.action_space()\n",
    "        self.encoded_actions = np.eye(self.size*self.size)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.ones(self.size*self.size)\n",
    "        return self.board.copy()\n",
    "    \n",
    "    def sample_game(self, shuffles):\n",
    "        self.reset()\n",
    "        next_state = self.board.copy()\n",
    "        action = random.choice(list(self.actions.keys()))\n",
    "        state, _, _, _ = self.step(action)\n",
    "        replay_buffer = []\n",
    "        done, score, reward = True, 0, 0\n",
    "        for i in range(random.randint(4, shuffles)):\n",
    "            replay_buffer.append((state, action, next_state, score, done, reward))\n",
    "            next_state = state\n",
    "            action = random.choice(list(self.actions.keys()))\n",
    "            state, reward, done, _ = self.step(action)\n",
    "            score += reward\n",
    "        replay_buffer.append((state, action, next_state, score, done, reward))\n",
    "        return self.board.copy(), list(reversed(replay_buffer))\n",
    "        \n",
    "    def step(self, action):\n",
    "        x, y = self.action_to_coordinate(action)\n",
    "        self.board[action] = not self.board[action]\n",
    "        if x - 1 >= 0:\n",
    "            index = self.size * (x - 1) + y\n",
    "            self.board[index] = not self.board[index]\n",
    "        if y - 1 >= 0:\n",
    "            index = self.size * x + y - 1\n",
    "            self.board[index] = not self.board[index]\n",
    "        if x + 1 < self.size:\n",
    "            index = self.size * (x + 1) + y\n",
    "            self.board[index] = not self.board[index]\n",
    "        if y + 1 < self.size:\n",
    "            index = self.size * x + y + 1\n",
    "            self.board[index] = not self.board[index]\n",
    "        done, reward = self.complete()\n",
    "        return self.board.copy(), reward, done, 0    \n",
    "            \n",
    "    def show_board(self):\n",
    "        print(\"----\")\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                print(self.board[self.size * i + j], end=\"\\t\")\n",
    "            print()\n",
    "            \n",
    "    def action_to_coordinate(self, action):\n",
    "        if action <= 15 and action >= 0:\n",
    "            return self.actions[action]\n",
    "        else:\n",
    "            print(\"Actions is not present\")\n",
    "            return -1\n",
    "            \n",
    "    def action_space(self):\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                self.actions[self.size*i + j] = (i, j)\n",
    "                \n",
    "    def complete(self):\n",
    "        for cell in self.board:\n",
    "            if cell == False:\n",
    "                return False, -1\n",
    "        return True, 0\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        self.board = state.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c77194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "0.0\t1.0\t0.0\t0.0\t\n",
      "1.0\t0.0\t0.0\t0.0\t\n",
      "0.0\t0.0\t0.0\t1.0\t\n",
      "0.0\t1.0\t0.0\t0.0\t\n",
      "----\n",
      "1.0\t1.0\t1.0\t1.0\t\n",
      "1.0\t1.0\t1.0\t1.0\t\n",
      "1.0\t1.0\t1.0\t1.0\t\n",
      "1.0\t1.0\t1.0\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "env = TacoYaki()\n",
    "state, replay_buffer = env.sample_game(50)\n",
    "env.show_board()\n",
    "for event in replay_buffer:\n",
    "    state, action, next_state, score, done, reward = event\n",
    "    env.step(action)\n",
    "# print(torch.argmax(actor.act(state)).item(), actor.act(state))\n",
    "env.show_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf42c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the neural network here\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.fc2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.fc3 = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the forward propagation\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a678dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                 max_mem_size=100000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        self.Q_eval = NeuralNet(input_dims, 256, n_actions).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.Q_eval.parameters(), lr=self.lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, input_dims),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_dims),\n",
    "                                         dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self.epsilon:  # with probability eps, the agent selects a random action\n",
    "            action = np.random.choice(self.action_space)\n",
    "            return action\n",
    "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "                q_values = self.Q_eval(state_tensor)\n",
    "                action = torch.argmax(q_values)\n",
    "            return action.item()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = torch.tensor(self.state_memory[batch]).to(device)\n",
    "        new_state_batch = torch.tensor(\n",
    "                self.new_state_memory[batch]).to(device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = torch.tensor(\n",
    "                self.reward_memory[batch]).to(device)\n",
    "        terminal_batch = torch.tensor(\n",
    "                self.terminal_memory[batch]).to(device)\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*torch.max(q_next, dim=1)[0]\n",
    "\n",
    "        loss = self.loss(q_target, q_eval)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.iter_cntr += 1\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "            if self.epsilon > self.eps_min else self.eps_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ccedc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GHOSH\\anaconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\GHOSH\\anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\GHOSH\\AppData\\Local\\Temp\\ipykernel_24724\\3264486090.py:26: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 avg score: -60.606841096082874 epsilon: 0.9980000000000002\n",
      "episode 1 avg score: -137.6427314358209 epsilon: 0.9605000000000044\n",
      "episode 2 avg score: -125.50977093894399 epsilon: 0.9240000000000084\n",
      "episode 3 avg score: -112.63591761784154 epsilon: 0.8905000000000121\n",
      "episode 4 avg score: -170.23707796393586 epsilon: 0.8145000000000204\n",
      "episode 5 avg score: -188.57949826968377 epsilon: 0.755000000000027\n",
      "episode 6 avg score: -168.50925875354636 epsilon: 0.6995000000000331\n",
      "episode 7 avg score: -162.15045699763337 epsilon: 0.6565000000000378\n",
      "episode 8 avg score: -147.8887411461602 epsilon: 0.6030000000000437\n",
      "episode 9 avg score: -132.93227015100564 epsilon: 0.5595000000000485\n",
      "episode 10 avg score: -124.83404153671391 epsilon: 0.48700000000005506\n",
      "episode 11 avg score: -118.77524387152037 epsilon: 0.431500000000055\n",
      "episode 12 avg score: -112.4127544181383 epsilon: 0.36800000000005495\n",
      "episode 13 avg score: -106.45850511955585 epsilon: 0.2025000000000548\n",
      "episode 14 avg score: -101.07698230234747 epsilon: 0.11150000000005472\n",
      "episode 15 avg score: -95.0439215193382 epsilon: 0.01\n",
      "episode 16 avg score: -96.18170820773825 epsilon: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     15\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_transition(observation,  action, reward, observation_, done)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation_\n\u001b[0;32m     18\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_memory[batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     66\u001b[0m terminal_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal_memory[batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 69\u001b[0m q_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m[batch_index, action_batch]\n\u001b[0;32m     70\u001b[0m q_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_eval\u001b[38;5;241m.\u001b[39mforward(new_state_batch)\n\u001b[0;32m     71\u001b[0m q_next[terminal_batch] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\" CODE HERE:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m        Implement the forward propagation\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "agent = Agent(gamma = 0.99, epsilon = 1.0, lr = 0.003, input_dims=8, batch_size = 64, n_actions= 4)\n",
    "\n",
    "scores , eps_history = deque(maxlen = 100), deque(maxlen = 100)\n",
    "n_games = 5000\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = agent.get_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        agent.store_transition(observation,  action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "        \n",
    "    avg_score = np.mean(scores)\n",
    "        \n",
    "    print('episode', i, 'avg score:', avg_score, 'epsilon:', agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ce1449",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TacoYaki()\n",
    "\n",
    "\n",
    "mem_size = 40000\n",
    "learning_rate= 0.001\n",
    "solved_score = 300\n",
    "gamma = 0.99\n",
    "batch_size = 10000\n",
    "episodes = 10000\n",
    "shuffles = 30\n",
    "\n",
    "action_num = 16\n",
    "hidden_dim = 265\n",
    "observation_dim = 16\n",
    "\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "\n",
    "target_net = NeuralNet(observation_dim, hidden_dim, action_num).to(device)\n",
    "\n",
    "# behavior_net.load_state_dict(target_net.state_dict())\n",
    "\n",
    "optimizer= torch.optim.Adam(target_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db4f10a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.326125144958496\n",
      "mean step loss in last 1000 steps  0.0004837204302020837\n",
      "\n",
      "New episode 1\n",
      "17.278133392333984\n",
      "mean step loss in last 1000 steps  0.000515870480958256\n",
      "\n",
      "New episode 2\n",
      "0.19495660066604614\n",
      "mean step loss in last 1000 steps  0.00032464720989082706\n",
      "\n",
      "New episode 3\n",
      "0.5273777842521667\n",
      "mean step loss in last 1000 steps  0.00011706209115800448\n",
      "\n",
      "New episode 4\n",
      "4.591670036315918\n",
      "mean step loss in last 1000 steps  0.0002784270903657671\n",
      "\n",
      "New episode 5\n",
      "1.1491751670837402\n",
      "mean step loss in last 1000 steps  0.0001620093733436079\n",
      "\n",
      "New episode 6\n",
      "5.705254077911377\n",
      "mean step loss in last 1000 steps  0.33008355786427274\n",
      "\n",
      "New episode 7\n",
      "0.00014959646796341985\n",
      "mean step loss in last 1000 steps  0.00011245207373121958\n",
      "\n",
      "New episode 8\n",
      "1.0728830375228426e-06\n",
      "mean step loss in last 1000 steps  0.00017271583827096037\n",
      "\n",
      "New episode 9\n",
      "18.696380615234375\n",
      "mean step loss in last 1000 steps  0.2521622660702269\n",
      "\n",
      "New episode 10\n",
      "11.186315536499023\n",
      "mean step loss in last 1000 steps  0.2683446784286207\n",
      "\n",
      "New episode 11\n",
      "0.08688168972730637\n",
      "mean step loss in last 1000 steps  0.00022997449575996143\n",
      "\n",
      "New episode 12\n",
      "4.861031532287598\n",
      "mean step loss in last 1000 steps  0.0003303300910783946\n",
      "\n",
      "New episode 13\n",
      "1.3432576656341553\n",
      "mean step loss in last 1000 steps  0.337341535351261\n",
      "\n",
      "New episode 14\n",
      "10.997345924377441\n",
      "mean step loss in last 1000 steps  0.000239597527688602\n",
      "\n",
      "New episode 15\n",
      "3.166903257369995\n",
      "mean step loss in last 1000 steps  0.00023203939359973448\n",
      "\n",
      "New episode 16\n",
      "3.3833866119384766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m CELoss(pred_actions, target)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# implementation of Deep value Iteration\n",
    "recent_losses = deque(maxlen = 100)\n",
    "testing = []\n",
    "\n",
    "buffers = []\n",
    "for ep in range(episodes):\n",
    "    state, replay_buffer = env.sample_game(5)\n",
    "    mean_loss = 100\n",
    "    buffers.append(replay_buffer)\n",
    "    steps = 0\n",
    "    while mean_loss > 1:\n",
    "        state, action, next_state, score, done, reward = random.choice(replay_buffer)\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        testing.append(state)\n",
    "        pred_actions = target_net.forward(state_tensor)\n",
    "        target = torch.from_numpy(env.encoded_actions[action]).reshape(1,-1).float().to(device)\n",
    "#         print(pred_actions, target)\n",
    "        loss = CELoss(pred_actions, target)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps == 0:\n",
    "            print(loss.item())\n",
    "        \n",
    "        recent_losses.append(loss.item())\n",
    "        steps += 1\n",
    "        \n",
    "        if (steps+1) % 1000 == 0:\n",
    "            mean_loss = np.array(recent_losses).mean()\n",
    "            print(\"mean step loss in last 1000 steps \",mean_loss)\n",
    "    print(\"\\n\" + \"New episode \" + str(ep+1))\n",
    "\n",
    "# for buffer in buffers:\n",
    "#     print()\n",
    "#     for event in buffer:\n",
    "#         state, action, next_state, score, done, reward = event\n",
    "#         act= torch.argmax(target_net(state))\n",
    "#         print(action, act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39810679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5835.0 1\n",
      "4518.5 2\n",
      "3202.3333333333335 3\n",
      "2550.25 4\n",
      "2309.0 5\n",
      "3471.8333333333335 6\n",
      "3541.5714285714284 7\n",
      "3099.125 8\n",
      "3478.5555555555557 9\n",
      "3130.9 10\n",
      "2990.4545454545455 11\n",
      "3010.25 12\n",
      "3003.0 13\n",
      "3624.9285714285716 14\n",
      "4175.4 15\n",
      "4190.6875 16\n",
      "4780.529411764706 17\n",
      "4962.055555555556 18\n",
      "4768.473684210527 19\n",
      "4552.65 20\n",
      "4489.952380952381 21\n",
      "4367.227272727273 22\n",
      "4230.304347826087 23\n",
      "4201.375 24\n",
      "4057.32 25\n",
      "3921.1153846153848 26\n",
      "3807.962962962963 27\n",
      "3801.1785714285716 28\n",
      "3776.5172413793102 29\n",
      "3666.3 30\n",
      "3590.7419354838707 31\n",
      "3564.78125 32\n",
      "3456.818181818182 33\n",
      "3452.1470588235293 34\n",
      "3591.6285714285714 35\n",
      "3494.75 36\n",
      "3428.0810810810813 37\n",
      "3341.7105263157896 38\n",
      "3365.4102564102564 39\n",
      "3372.175 40\n",
      "3289.9756097560976 41\n",
      "3309.2619047619046 42\n",
      "3522.1627906976746 43\n",
      "3564.431818181818 44\n",
      "3485.266666666667 45\n",
      "3478.0652173913045 46\n",
      "3408.1063829787236 47\n",
      "3446.9375 48\n",
      "3437.3673469387754 49\n",
      "3392.42 50\n",
      "3325.9411764705883 51\n",
      "3427.826923076923 52\n",
      "3404.622641509434 53\n",
      "3345.796296296296 54\n",
      "3285.0 55\n",
      "3262.625 56\n",
      "3211.8070175438597 57\n",
      "3175.155172413793 58\n",
      "3146.6949152542375 59\n",
      "3125.75 60\n",
      "3135.7868852459014 61\n",
      "3135.8548387096776 62\n",
      "3091.15873015873 63\n",
      "3133.171875 64\n",
      "3120.076923076923 65\n",
      "3072.8333333333335 66\n",
      "3034.5522388059703 67\n",
      "3271.1029411764707 68\n",
      "3277.463768115942 69\n",
      "3252.7571428571428 70\n",
      "3282.4647887323945 71\n",
      "3236.902777777778 72\n",
      "3247.1643835616437 73\n",
      "3367.5810810810813 74\n",
      "3400.44 75\n",
      "3363.8026315789475 76\n",
      "3338.87012987013 77\n",
      "3296.0897435897436 78\n",
      "3254.746835443038 79\n",
      "3219.0875 80\n",
      "3228.6543209876545 81\n",
      "3202.7926829268295 82\n",
      "3217.265060240964 83\n",
      "3240.3214285714284 84\n",
      "3241.0 85\n",
      "3244.8953488372094 86\n",
      "3226.5402298850577 87\n",
      "3265.3977272727275 88\n",
      "3232.955056179775 89\n",
      "3197.1222222222223 90\n",
      "3198.2747252747254 91\n",
      "3202.728260869565 92\n",
      "3168.3118279569894 93\n",
      "3158.968085106383 94\n",
      "3131.2315789473682 95\n",
      "3147.9479166666665 96\n",
      "3142.0515463917527 97\n",
      "3210.3571428571427 98\n",
      "3230.6363636363635 99\n",
      "3273.41 100\n",
      "3307.3564356435645 101\n",
      "3277.1470588235293 102\n",
      "3295.4854368932038 103\n",
      "3272.4326923076924 104\n",
      "3291.8571428571427 105\n",
      "3352.122641509434 106\n",
      "3321.467289719626 107\n",
      "3297.3611111111113 108\n",
      "3346.743119266055 109\n",
      "3328.4454545454546 110\n",
      "3326.90990990991 111\n",
      "3305.2410714285716 112\n",
      "3291.477876106195 113\n",
      "3296.2719298245615 114\n",
      "3267.6260869565217 115\n",
      "3281.8362068965516 116\n",
      "3272.008547008547 117\n",
      "3336.3474576271187 118\n",
      "3333.873949579832 119\n",
      "3306.608333333333 120\n",
      "3282.404958677686 121\n",
      "3271.6311475409834 122\n",
      "3255.0975609756097 123\n",
      "3246.3951612903224 124\n",
      "3262.68 125\n",
      "3259.1666666666665 126\n",
      "3261.6929133858266 127\n",
      "3351.6328125 128\n",
      "3365.232558139535 129\n",
      "3365.2384615384617 130\n",
      "3352.8931297709923 131\n",
      "3328.037878787879 132\n",
      "3325.4511278195487 133\n",
      "3325.216417910448 134\n",
      "3311.3259259259257 135\n",
      "3299.5220588235293 136\n",
      "3289.204379562044 137\n",
      "3339.6739130434785 138\n",
      "3330.021582733813 139\n",
      "3306.25 140\n",
      "3359.1702127659573 141\n",
      "3335.950704225352 142\n",
      "3353.153846153846 143\n",
      "3335.0625 144\n",
      "3323.88275862069 145\n",
      "3311.9794520547944 146\n",
      "3326.8503401360545 147\n",
      "3347.4662162162163 148\n",
      "3330.087248322148 149\n",
      "3310.5266666666666 150\n",
      "3411.543046357616 151\n",
      "3446.1644736842104 152\n",
      "3430.267973856209 153\n",
      "3421.512987012987 154\n",
      "3399.451612903226 155\n",
      "3377.673076923077 156\n",
      "3371.5859872611463 157\n",
      "3405.3734177215188 158\n",
      "3406.245283018868 159\n",
      "3412.19375 160\n",
      "3448.950310559006 161\n",
      "3532.7716049382716 162\n",
      "3524.60736196319 163\n",
      "3528.810975609756 164\n",
      "3508.2484848484846 165\n",
      "3500.7168674698796 166\n",
      "3536.604790419162 167\n",
      "3531.7321428571427 168\n",
      "3655.792899408284 169\n",
      "3664.1117647058823 170\n",
      "3642.6959064327484 171\n",
      "3621.5523255813955 172\n",
      "3608.6300578034684 173\n",
      "3600.270114942529 174\n",
      "3589.3885714285716 175\n",
      "3590.505681818182 176\n",
      "3592.2881355932204 177\n",
      "3590.9157303370785 178\n",
      "3587.0335195530724 179\n",
      "3578.516666666667 180\n",
      "3573.696132596685 181\n",
      "3586.620879120879 182\n",
      "3609.2295081967213 183\n",
      "3601.266304347826 184\n",
      "3581.810810810811 185\n",
      "3579.2311827956987 186\n",
      "3624.133689839572 187\n",
      "3713.590425531915 188\n",
      "3695.5714285714284 189\n",
      "3715.8473684210526 190\n",
      "3711.083769633508 191\n",
      "3717.4114583333335 192\n",
      "3711.5492227979275 193\n",
      "3694.7061855670104 194\n",
      "3675.769230769231 195\n",
      "3665.25 196\n",
      "3661.8832487309646 197\n",
      "3658.469696969697 198\n",
      "3648.5577889447236 199\n",
      "3658.385 200\n",
      "3658.741293532338 201\n",
      "3760.6485148514853 202\n",
      "3742.1330049261082 203\n",
      "3761.406862745098 204\n",
      "3755.5463414634146 205\n",
      "3820.1019417475727 206\n",
      "3809.1932367149757 207\n",
      "3790.889423076923 208\n",
      "3779.8325358851675 209\n",
      "3766.5857142857144 210\n",
      "3751.379146919431 211\n",
      "3756.419811320755 212\n",
      "3759.507042253521 213\n",
      "3747.303738317757 214\n",
      "3734.2930232558138 215\n",
      "3718.800925925926 216\n",
      "3745.9400921658985 217\n",
      "3744.729357798165 218\n",
      "3728.780821917808 219\n",
      "3731.668181818182 220\n",
      "3737.5339366515836 221\n",
      "3731.887387387387 222\n",
      "3726.919282511211 223\n",
      "3711.1651785714284 224\n",
      "3699.6311111111113 225\n",
      "3768.287610619469 226\n",
      "3762.453744493392 227\n",
      "3792.34649122807 228\n",
      "3790.9039301310045 229\n",
      "3813.813043478261 230\n",
      "3805.095238095238 231\n",
      "3789.5991379310344 232\n",
      "3782.3390557939915 233\n",
      "3776.2264957264956 234\n",
      "3806.6510638297873 235\n",
      "3790.834745762712 236\n",
      "3779.3881856540083 237\n",
      "3811.0378151260506 238\n",
      "3795.242677824268 239\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      7\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m----> 8\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     10\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mTacoYaki.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard[index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard[index]\n\u001b[0;32m     44\u001b[0m done, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomplete()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, reward, done, \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state, _ = env.sample_game(5)\n",
    "scores = []\n",
    "for i in range(500):\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "        score += 1\n",
    "    scores.append(score)\n",
    "    print(np.mean(scores), len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e85c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
