{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ad6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c288bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c7ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacoYaki():\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.board = np.ones(self.size*self.size)\n",
    "        self.actions = {}\n",
    "        self.action_space()\n",
    "        self.encoded_actions = np.eye(self.size*self.size)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.ones(self.size*self.size)\n",
    "    \n",
    "    def sample_game(self, shuffles):\n",
    "        self.reset()\n",
    "        next_state = self.board.copy()\n",
    "        action = random.choice(list(self.actions.keys()))\n",
    "        state, _, _, _ = self.step(action)\n",
    "        replay_buffer = []\n",
    "        done, score, reward = True, 0, 0\n",
    "        for i in range(random.randint(4, shuffles)):\n",
    "            replay_buffer.append((state, action, next_state, score, done, reward))\n",
    "            next_state = state\n",
    "            action = random.choice(list(self.actions.keys()))\n",
    "            state, reward, done, _ = self.step(action)\n",
    "            score += reward\n",
    "        replay_buffer.append((state, action, next_state, score, done, reward))\n",
    "        return self.board, list(reversed(replay_buffer))\n",
    "        \n",
    "    def step(self, action):\n",
    "        x, y = self.action_to_coordinate(action)\n",
    "        self.board[action] = not self.board[action]\n",
    "        if x - 1 >= 0:\n",
    "            index = self.size * (x - 1) + y\n",
    "            self.board[index] = not self.board[index]\n",
    "        if y - 1 >= 0:\n",
    "            index = self.size * x + y - 1\n",
    "            self.board[index] = not self.board[index]\n",
    "        if x + 1 < self.size:\n",
    "            index = self.size * (x + 1) + y\n",
    "            self.board[index] = not self.board[index]\n",
    "        if y + 1 < self.size:\n",
    "            index = self.size * x + y + 1\n",
    "            self.board[index] = not self.board[index]\n",
    "        done, reward = self.complete()\n",
    "        return self.board.copy(), reward, done, 0    \n",
    "            \n",
    "    def show_board(self):\n",
    "        print(\"----\")\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                print(self.board[self.size * i + j], end=\"\\t\")\n",
    "            print()\n",
    "            \n",
    "    def action_to_coordinate(self, action):\n",
    "        if action <= 15 and action >= 0:\n",
    "            return self.actions[action]\n",
    "        else:\n",
    "            print(\"Actions is not present\")\n",
    "            return -1\n",
    "            \n",
    "    def action_space(self):\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                self.actions[self.size*i + j] = (i, j)\n",
    "                \n",
    "    def complete(self):\n",
    "        for cell in self.board:\n",
    "            if cell == False:\n",
    "                return False, -1\n",
    "        return True, 0\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        self.board = state.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c77194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "1.0\t1.0\t1.0\t1.0\t\n",
      "1.0\t0.0\t1.0\t0.0\t\n",
      "1.0\t1.0\t1.0\t1.0\t\n",
      "1.0\t0.0\t1.0\t1.0\t\n",
      "----\n",
      "1.0\t1.0\t1.0\t1.0\t\n",
      "1.0\t1.0\t1.0\t1.0\t\n",
      "1.0\t1.0\t1.0\t1.0\t\n",
      "1.0\t1.0\t1.0\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "env = TacoYaki()\n",
    "state, replay_buffer = env.sample_game(50)\n",
    "env.show_board()\n",
    "for event in replay_buffer:\n",
    "    state, action, next_state, score, done, reward = event\n",
    "    env.step(action)\n",
    "# print(torch.argmax(actor.act(state)).item(), actor.act(state))\n",
    "env.show_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf42c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the neural network here\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.fc2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.fc3 = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the forward propagation\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a678dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                 max_mem_size=100000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        self.Q_eval = NeuralNet(input_dims, 256, n_actions).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.Q_eval.parameters(), lr=self.lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, input_dims),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_dims),\n",
    "                                         dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self.epsilon:  # with probability eps, the agent selects a random action\n",
    "            action = np.random.choice(self.action_space)\n",
    "            return action\n",
    "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "                q_values = self.Q_eval(state_tensor)\n",
    "                action = torch.argmax(q_values)\n",
    "            return action.item()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = torch.tensor(self.state_memory[batch]).to(device)\n",
    "        new_state_batch = torch.tensor(\n",
    "                self.new_state_memory[batch]).to(device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = torch.tensor(\n",
    "                self.reward_memory[batch]).to(device)\n",
    "        terminal_batch = torch.tensor(\n",
    "                self.terminal_memory[batch]).to(device)\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*torch.max(q_next, dim=1)[0]\n",
    "\n",
    "        loss = self.loss(q_target, q_eval)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.iter_cntr += 1\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "            if self.epsilon > self.eps_min else self.eps_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75404dc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GHOSH\\AppData\\Local\\Temp\\ipykernel_40372\\3264486090.py:26: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 avg score: -129.68257655489668 epsilon: 0.9980000000000002\n",
      "episode 1 avg score: -92.9987382157642 epsilon: 0.9645000000000039\n",
      "episode 2 avg score: -133.31439359162002 epsilon: 0.9265000000000081\n",
      "episode 3 avg score: -121.15118697313645 epsilon: 0.8870000000000124\n",
      "episode 4 avg score: -114.66236917440946 epsilon: 0.8435000000000172\n",
      "episode 5 avg score: -107.91646441191646 epsilon: 0.800000000000022\n",
      "episode 6 avg score: -107.59517768870307 epsilon: 0.7510000000000274\n",
      "episode 7 avg score: -141.8807530253879 epsilon: 0.6905000000000341\n",
      "episode 8 avg score: -136.5952044299328 epsilon: 0.628000000000041\n",
      "episode 9 avg score: -134.6780522549261 epsilon: 0.5650000000000479\n",
      "episode 10 avg score: -123.94922716135926 epsilon: 0.49350000000005506\n",
      "episode 11 avg score: -158.32210283317215 epsilon: 0.3110000000000549\n",
      "episode 12 avg score: -144.4584248645459 epsilon: 0.24100000000005484\n",
      "episode 13 avg score: -131.84444251947406 epsilon: 0.14850000000005475\n",
      "episode 14 avg score: -104.75230049543302 epsilon: 0.01\n",
      "episode 15 avg score: -98.12951709025361 epsilon: 0.01\n",
      "episode 16 avg score: -93.74421423187009 epsilon: 0.01\n",
      "episode 17 avg score: -92.16218841236986 epsilon: 0.01\n",
      "episode 18 avg score: -86.22777123039637 epsilon: 0.01\n",
      "episode 19 avg score: -84.51089896539196 epsilon: 0.01\n",
      "episode 20 avg score: -89.64371631475501 epsilon: 0.01\n",
      "episode 21 avg score: -90.42271581621091 epsilon: 0.01\n",
      "episode 22 avg score: -87.99859230103368 epsilon: 0.01\n",
      "episode 23 avg score: -87.55179343702076 epsilon: 0.01\n",
      "episode 24 avg score: -86.26913790568055 epsilon: 0.01\n",
      "episode 25 avg score: -82.8636605708433 epsilon: 0.01\n",
      "episode 26 avg score: -79.45561535329134 epsilon: 0.01\n",
      "episode 27 avg score: -72.34088608735945 epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "agent = Agent(gamma = 0.99, epsilon = 1.0, lr = 0.003, input_dims=8, batch_size = 64, n_actions= 4)\n",
    "\n",
    "scores , eps_history = deque(maxlen = 100), deque(maxlen = 100)\n",
    "n_games = 5000\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = agent.get_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        agent.store_transition(observation,  action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "        \n",
    "    avg_score = np.mean(scores)\n",
    "        \n",
    "    print('episode', i, 'avg score:', avg_score, 'epsilon:', agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66ccedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TacoYaki()\n",
    "\n",
    "\n",
    "mem_size = 40000\n",
    "learning_rate= 0.001\n",
    "solved_score = 300\n",
    "gamma = 0.99\n",
    "batch_size = 10000\n",
    "episodes = 10000\n",
    "shuffles = 30\n",
    "\n",
    "action_num = 16\n",
    "hidden_dim = 265\n",
    "observation_dim = 16\n",
    "\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "\n",
    "target_net = NeuralNet(observation_dim, hidden_dim, action_num).to(device)\n",
    "\n",
    "# behavior_net.load_state_dict(target_net.state_dict())\n",
    "\n",
    "optimizer= torch.optim.Adam(target_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74ce1449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7840230464935303\n",
      "mean step loss in last 1000 steps  0.0005283898013294674\n",
      "\n",
      "New episode 1\n",
      "5.7897162437438965\n",
      "mean step loss in last 1000 steps  0.0004430137446615845\n",
      "\n",
      "New episode 2\n",
      "11.006105422973633\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m CELoss(pred_actions, target)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# implementation of Deep value Iteration\n",
    "recent_losses = deque(maxlen = 100)\n",
    "testing = []\n",
    "\n",
    "buffers = []\n",
    "for ep in range(episodes):\n",
    "    state, replay_buffer = env.sample_game(5)\n",
    "    mean_loss = 100\n",
    "    buffers.append(replay_buffer)\n",
    "    steps = 0\n",
    "    while mean_loss > 1:\n",
    "        state, action, next_state, score, done, reward = random.choice(replay_buffer)\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        testing.append(state)\n",
    "        pred_actions = target_net.forward(state_tensor)\n",
    "        target = torch.from_numpy(env.encoded_actions[action]).reshape(1,-1).float().to(device)\n",
    "#         print(pred_actions, target)\n",
    "        loss = CELoss(pred_actions, target)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps == 0:\n",
    "            print(loss.item())\n",
    "        \n",
    "        recent_losses.append(loss.item())\n",
    "        steps += 1\n",
    "        \n",
    "        if (steps+1) % 1000 == 0:\n",
    "            mean_loss = np.array(recent_losses).mean()\n",
    "            print(\"mean step loss in last 1000 steps \",mean_loss)\n",
    "    print(\"\\n\" + \"New episode \" + str(ep+1))\n",
    "\n",
    "# for buffer in buffers:\n",
    "#     print()\n",
    "#     for event in buffer:\n",
    "#         state, action, next_state, score, done, reward = event\n",
    "#         act= torch.argmax(target_net(state))\n",
    "#         print(action, act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f10a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
